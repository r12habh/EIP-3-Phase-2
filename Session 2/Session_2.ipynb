{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO-wO_bpX-4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8f30329-01fc-4e0a-e671-46713490bae2"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import string\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM1eEORpXYUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2215f09e-54cb-44a0-8d74-a7abfc4d6f27"
      },
      "source": [
        "link = \"http://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
        "r = requests.get(link, allow_redirects=True)\n",
        "open('wonderland.txt', 'wb').write(r.content)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167518"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA6lQM4kZPrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbXl89W6ZG4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = raw_text.translate(str.maketrans('','',string.punctuation))\n",
        "raw_text = raw_text.translate(str.maketrans(\"\\n\",' ',string.punctuation))\n",
        "raw_text = raw_text[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG3fCaQSZHhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcN5_qjoascs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f82202b-5918-4afb-927e-7314000d5859"
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print( \"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  154860\n",
            "Total Vocab:  37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIlMn-nsbKoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c49f2d25-2d61-4ef5-e8ed-2f1f55a9a5ec"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 50\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  154810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrYgo2q0bNR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "57fae7f1-0a05-45c5-e106-e7eb3a12edbd",
        "id": "MPT1o38Meosh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 07:30:49.512671 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0728 07:30:49.551553 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 07:30:49.558970 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0728 07:30:49.789352 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0728 07:30:49.797816 139864107091840 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 50, 128)           66560     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 37)                4773      \n",
            "=================================================================\n",
            "Total params: 202,917\n",
            "Trainable params: 202,917\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR4lFMAabeGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a89e268-268d-45e5-de5a-b32a1785a72f"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0728 07:30:50.023153 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 07:30:50.049316 139864107091840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0728 07:30:50.180960 139864107091840 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "154810/154810 [==============================] - 82s 530us/step - loss: 2.7906\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.79059, saving model to weights-improvement-01-2.7906-bigger.hdf5\n",
            "Epoch 2/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 2.5493\n",
            "\n",
            "Epoch 00002: loss improved from 2.79059 to 2.54927, saving model to weights-improvement-02-2.5493-bigger.hdf5\n",
            "Epoch 3/100\n",
            "154810/154810 [==============================] - 77s 501us/step - loss: 2.4283\n",
            "\n",
            "Epoch 00003: loss improved from 2.54927 to 2.42827, saving model to weights-improvement-03-2.4283-bigger.hdf5\n",
            "Epoch 4/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 2.3163\n",
            "\n",
            "Epoch 00004: loss improved from 2.42827 to 2.31635, saving model to weights-improvement-04-2.3163-bigger.hdf5\n",
            "Epoch 5/100\n",
            "154810/154810 [==============================] - 77s 501us/step - loss: 2.2307\n",
            "\n",
            "Epoch 00005: loss improved from 2.31635 to 2.23070, saving model to weights-improvement-05-2.2307-bigger.hdf5\n",
            "Epoch 6/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 2.1640\n",
            "\n",
            "Epoch 00006: loss improved from 2.23070 to 2.16405, saving model to weights-improvement-06-2.1640-bigger.hdf5\n",
            "Epoch 7/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 2.1080\n",
            "\n",
            "Epoch 00007: loss improved from 2.16405 to 2.10803, saving model to weights-improvement-07-2.1080-bigger.hdf5\n",
            "Epoch 8/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 2.0623\n",
            "\n",
            "Epoch 00008: loss improved from 2.10803 to 2.06229, saving model to weights-improvement-08-2.0623-bigger.hdf5\n",
            "Epoch 9/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 2.0203\n",
            "\n",
            "Epoch 00009: loss improved from 2.06229 to 2.02026, saving model to weights-improvement-09-2.0203-bigger.hdf5\n",
            "Epoch 10/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.9833\n",
            "\n",
            "Epoch 00010: loss improved from 2.02026 to 1.98328, saving model to weights-improvement-10-1.9833-bigger.hdf5\n",
            "Epoch 11/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.9500\n",
            "\n",
            "Epoch 00011: loss improved from 1.98328 to 1.95000, saving model to weights-improvement-11-1.9500-bigger.hdf5\n",
            "Epoch 12/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.9201\n",
            "\n",
            "Epoch 00012: loss improved from 1.95000 to 1.92011, saving model to weights-improvement-12-1.9201-bigger.hdf5\n",
            "Epoch 13/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.8923\n",
            "\n",
            "Epoch 00013: loss improved from 1.92011 to 1.89231, saving model to weights-improvement-13-1.8923-bigger.hdf5\n",
            "Epoch 14/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.8672\n",
            "\n",
            "Epoch 00014: loss improved from 1.89231 to 1.86723, saving model to weights-improvement-14-1.8672-bigger.hdf5\n",
            "Epoch 15/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.8414\n",
            "\n",
            "Epoch 00015: loss improved from 1.86723 to 1.84136, saving model to weights-improvement-15-1.8414-bigger.hdf5\n",
            "Epoch 16/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.8185\n",
            "\n",
            "Epoch 00016: loss improved from 1.84136 to 1.81847, saving model to weights-improvement-16-1.8185-bigger.hdf5\n",
            "Epoch 17/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.7974\n",
            "\n",
            "Epoch 00017: loss improved from 1.81847 to 1.79743, saving model to weights-improvement-17-1.7974-bigger.hdf5\n",
            "Epoch 18/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.7766\n",
            "\n",
            "Epoch 00018: loss improved from 1.79743 to 1.77662, saving model to weights-improvement-18-1.7766-bigger.hdf5\n",
            "Epoch 19/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.7564\n",
            "\n",
            "Epoch 00019: loss improved from 1.77662 to 1.75636, saving model to weights-improvement-19-1.7564-bigger.hdf5\n",
            "Epoch 20/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.7379\n",
            "\n",
            "Epoch 00020: loss improved from 1.75636 to 1.73793, saving model to weights-improvement-20-1.7379-bigger.hdf5\n",
            "Epoch 21/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.7207\n",
            "\n",
            "Epoch 00021: loss improved from 1.73793 to 1.72073, saving model to weights-improvement-21-1.7207-bigger.hdf5\n",
            "Epoch 22/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.7043\n",
            "\n",
            "Epoch 00022: loss improved from 1.72073 to 1.70434, saving model to weights-improvement-22-1.7043-bigger.hdf5\n",
            "Epoch 23/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.6878\n",
            "\n",
            "Epoch 00023: loss improved from 1.70434 to 1.68780, saving model to weights-improvement-23-1.6878-bigger.hdf5\n",
            "Epoch 24/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.6712\n",
            "\n",
            "Epoch 00024: loss improved from 1.68780 to 1.67122, saving model to weights-improvement-24-1.6712-bigger.hdf5\n",
            "Epoch 25/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.6566\n",
            "\n",
            "Epoch 00025: loss improved from 1.67122 to 1.65659, saving model to weights-improvement-25-1.6566-bigger.hdf5\n",
            "Epoch 26/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.6436\n",
            "\n",
            "Epoch 00026: loss improved from 1.65659 to 1.64357, saving model to weights-improvement-26-1.6436-bigger.hdf5\n",
            "Epoch 27/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.6285\n",
            "\n",
            "Epoch 00027: loss improved from 1.64357 to 1.62845, saving model to weights-improvement-27-1.6285-bigger.hdf5\n",
            "Epoch 28/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.6162\n",
            "\n",
            "Epoch 00028: loss improved from 1.62845 to 1.61618, saving model to weights-improvement-28-1.6162-bigger.hdf5\n",
            "Epoch 29/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.6040\n",
            "\n",
            "Epoch 00029: loss improved from 1.61618 to 1.60399, saving model to weights-improvement-29-1.6040-bigger.hdf5\n",
            "Epoch 30/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.5919\n",
            "\n",
            "Epoch 00030: loss improved from 1.60399 to 1.59193, saving model to weights-improvement-30-1.5919-bigger.hdf5\n",
            "Epoch 31/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.5800\n",
            "\n",
            "Epoch 00031: loss improved from 1.59193 to 1.58000, saving model to weights-improvement-31-1.5800-bigger.hdf5\n",
            "Epoch 32/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.5659\n",
            "\n",
            "Epoch 00032: loss improved from 1.58000 to 1.56594, saving model to weights-improvement-32-1.5659-bigger.hdf5\n",
            "Epoch 33/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.5575\n",
            "\n",
            "Epoch 00033: loss improved from 1.56594 to 1.55748, saving model to weights-improvement-33-1.5575-bigger.hdf5\n",
            "Epoch 34/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.5454\n",
            "\n",
            "Epoch 00034: loss improved from 1.55748 to 1.54537, saving model to weights-improvement-34-1.5454-bigger.hdf5\n",
            "Epoch 35/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.5380\n",
            "\n",
            "Epoch 00035: loss improved from 1.54537 to 1.53796, saving model to weights-improvement-35-1.5380-bigger.hdf5\n",
            "Epoch 36/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.5272\n",
            "\n",
            "Epoch 00036: loss improved from 1.53796 to 1.52718, saving model to weights-improvement-36-1.5272-bigger.hdf5\n",
            "Epoch 37/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.5177\n",
            "\n",
            "Epoch 00037: loss improved from 1.52718 to 1.51768, saving model to weights-improvement-37-1.5177-bigger.hdf5\n",
            "Epoch 38/100\n",
            "154810/154810 [==============================] - 77s 494us/step - loss: 1.5078\n",
            "\n",
            "Epoch 00038: loss improved from 1.51768 to 1.50778, saving model to weights-improvement-38-1.5078-bigger.hdf5\n",
            "Epoch 39/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.4984\n",
            "\n",
            "Epoch 00039: loss improved from 1.50778 to 1.49835, saving model to weights-improvement-39-1.4984-bigger.hdf5\n",
            "Epoch 40/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.4906\n",
            "\n",
            "Epoch 00040: loss improved from 1.49835 to 1.49059, saving model to weights-improvement-40-1.4906-bigger.hdf5\n",
            "Epoch 41/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.4831\n",
            "\n",
            "Epoch 00041: loss improved from 1.49059 to 1.48308, saving model to weights-improvement-41-1.4831-bigger.hdf5\n",
            "Epoch 42/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.4748\n",
            "\n",
            "Epoch 00042: loss improved from 1.48308 to 1.47485, saving model to weights-improvement-42-1.4748-bigger.hdf5\n",
            "Epoch 43/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.4620\n",
            "\n",
            "Epoch 00043: loss improved from 1.47485 to 1.46199, saving model to weights-improvement-43-1.4620-bigger.hdf5\n",
            "Epoch 44/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.4582\n",
            "\n",
            "Epoch 00044: loss improved from 1.46199 to 1.45819, saving model to weights-improvement-44-1.4582-bigger.hdf5\n",
            "Epoch 45/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.4496\n",
            "\n",
            "Epoch 00045: loss improved from 1.45819 to 1.44958, saving model to weights-improvement-45-1.4496-bigger.hdf5\n",
            "Epoch 46/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.4422\n",
            "\n",
            "Epoch 00046: loss improved from 1.44958 to 1.44219, saving model to weights-improvement-46-1.4422-bigger.hdf5\n",
            "Epoch 47/100\n",
            "154810/154810 [==============================] - 76s 494us/step - loss: 1.4345\n",
            "\n",
            "Epoch 00047: loss improved from 1.44219 to 1.43453, saving model to weights-improvement-47-1.4345-bigger.hdf5\n",
            "Epoch 48/100\n",
            "154810/154810 [==============================] - 76s 489us/step - loss: 1.4265\n",
            "\n",
            "Epoch 00048: loss improved from 1.43453 to 1.42646, saving model to weights-improvement-48-1.4265-bigger.hdf5\n",
            "Epoch 49/100\n",
            "154810/154810 [==============================] - 76s 490us/step - loss: 1.4212\n",
            "\n",
            "Epoch 00049: loss improved from 1.42646 to 1.42121, saving model to weights-improvement-49-1.4212-bigger.hdf5\n",
            "Epoch 50/100\n",
            "154810/154810 [==============================] - 76s 489us/step - loss: 1.4136\n",
            "\n",
            "Epoch 00050: loss improved from 1.42121 to 1.41361, saving model to weights-improvement-50-1.4136-bigger.hdf5\n",
            "Epoch 51/100\n",
            "154810/154810 [==============================] - 76s 489us/step - loss: 1.4052\n",
            "\n",
            "Epoch 00051: loss improved from 1.41361 to 1.40524, saving model to weights-improvement-51-1.4052-bigger.hdf5\n",
            "Epoch 52/100\n",
            "154810/154810 [==============================] - 76s 488us/step - loss: 1.4010\n",
            "\n",
            "Epoch 00052: loss improved from 1.40524 to 1.40096, saving model to weights-improvement-52-1.4010-bigger.hdf5\n",
            "Epoch 53/100\n",
            "154810/154810 [==============================] - 80s 519us/step - loss: 1.3947\n",
            "\n",
            "Epoch 00053: loss improved from 1.40096 to 1.39471, saving model to weights-improvement-53-1.3947-bigger.hdf5\n",
            "Epoch 54/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 1.3881\n",
            "\n",
            "Epoch 00054: loss improved from 1.39471 to 1.38815, saving model to weights-improvement-54-1.3881-bigger.hdf5\n",
            "Epoch 55/100\n",
            "154810/154810 [==============================] - 78s 506us/step - loss: 1.3826\n",
            "\n",
            "Epoch 00055: loss improved from 1.38815 to 1.38261, saving model to weights-improvement-55-1.3826-bigger.hdf5\n",
            "Epoch 56/100\n",
            "154810/154810 [==============================] - 78s 505us/step - loss: 1.3760\n",
            "\n",
            "Epoch 00056: loss improved from 1.38261 to 1.37602, saving model to weights-improvement-56-1.3760-bigger.hdf5\n",
            "Epoch 57/100\n",
            "154810/154810 [==============================] - 78s 507us/step - loss: 1.3690\n",
            "\n",
            "Epoch 00057: loss improved from 1.37602 to 1.36899, saving model to weights-improvement-57-1.3690-bigger.hdf5\n",
            "Epoch 58/100\n",
            "154810/154810 [==============================] - 78s 506us/step - loss: 1.3640\n",
            "\n",
            "Epoch 00058: loss improved from 1.36899 to 1.36400, saving model to weights-improvement-58-1.3640-bigger.hdf5\n",
            "Epoch 59/100\n",
            "154810/154810 [==============================] - 78s 506us/step - loss: 1.3606\n",
            "\n",
            "Epoch 00059: loss improved from 1.36400 to 1.36059, saving model to weights-improvement-59-1.3606-bigger.hdf5\n",
            "Epoch 60/100\n",
            "154810/154810 [==============================] - 78s 505us/step - loss: 1.3564\n",
            "\n",
            "Epoch 00060: loss improved from 1.36059 to 1.35642, saving model to weights-improvement-60-1.3564-bigger.hdf5\n",
            "Epoch 61/100\n",
            "154810/154810 [==============================] - 78s 504us/step - loss: 1.3492\n",
            "\n",
            "Epoch 00061: loss improved from 1.35642 to 1.34923, saving model to weights-improvement-61-1.3492-bigger.hdf5\n",
            "Epoch 62/100\n",
            "154810/154810 [==============================] - 78s 505us/step - loss: 1.3434\n",
            "\n",
            "Epoch 00062: loss improved from 1.34923 to 1.34341, saving model to weights-improvement-62-1.3434-bigger.hdf5\n",
            "Epoch 63/100\n",
            "154810/154810 [==============================] - 78s 504us/step - loss: 1.3390\n",
            "\n",
            "Epoch 00063: loss improved from 1.34341 to 1.33903, saving model to weights-improvement-63-1.3390-bigger.hdf5\n",
            "Epoch 64/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 1.3341\n",
            "\n",
            "Epoch 00064: loss improved from 1.33903 to 1.33406, saving model to weights-improvement-64-1.3341-bigger.hdf5\n",
            "Epoch 65/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 1.3280\n",
            "\n",
            "Epoch 00065: loss improved from 1.33406 to 1.32803, saving model to weights-improvement-65-1.3280-bigger.hdf5\n",
            "Epoch 66/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 1.3228\n",
            "\n",
            "Epoch 00066: loss improved from 1.32803 to 1.32280, saving model to weights-improvement-66-1.3228-bigger.hdf5\n",
            "Epoch 67/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.3194\n",
            "\n",
            "Epoch 00067: loss improved from 1.32280 to 1.31939, saving model to weights-improvement-67-1.3194-bigger.hdf5\n",
            "Epoch 68/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.3140\n",
            "\n",
            "Epoch 00068: loss improved from 1.31939 to 1.31400, saving model to weights-improvement-68-1.3140-bigger.hdf5\n",
            "Epoch 69/100\n",
            "154810/154810 [==============================] - 78s 504us/step - loss: 1.3110\n",
            "\n",
            "Epoch 00069: loss improved from 1.31400 to 1.31098, saving model to weights-improvement-69-1.3110-bigger.hdf5\n",
            "Epoch 70/100\n",
            "154810/154810 [==============================] - 78s 504us/step - loss: 1.3087\n",
            "\n",
            "Epoch 00070: loss improved from 1.31098 to 1.30872, saving model to weights-improvement-70-1.3087-bigger.hdf5\n",
            "Epoch 71/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.3008\n",
            "\n",
            "Epoch 00071: loss improved from 1.30872 to 1.30079, saving model to weights-improvement-71-1.3008-bigger.hdf5\n",
            "Epoch 72/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.2976\n",
            "\n",
            "Epoch 00072: loss improved from 1.30079 to 1.29758, saving model to weights-improvement-72-1.2976-bigger.hdf5\n",
            "Epoch 73/100\n",
            "154810/154810 [==============================] - 78s 504us/step - loss: 1.2925\n",
            "\n",
            "Epoch 00073: loss improved from 1.29758 to 1.29252, saving model to weights-improvement-73-1.2925-bigger.hdf5\n",
            "Epoch 74/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.2907\n",
            "\n",
            "Epoch 00074: loss improved from 1.29252 to 1.29068, saving model to weights-improvement-74-1.2907-bigger.hdf5\n",
            "Epoch 75/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.2838\n",
            "\n",
            "Epoch 00075: loss improved from 1.29068 to 1.28375, saving model to weights-improvement-75-1.2838-bigger.hdf5\n",
            "Epoch 76/100\n",
            "154810/154810 [==============================] - 77s 501us/step - loss: 1.2798\n",
            "\n",
            "Epoch 00076: loss improved from 1.28375 to 1.27977, saving model to weights-improvement-76-1.2798-bigger.hdf5\n",
            "Epoch 77/100\n",
            "154810/154810 [==============================] - 78s 503us/step - loss: 1.2791\n",
            "\n",
            "Epoch 00077: loss improved from 1.27977 to 1.27912, saving model to weights-improvement-77-1.2791-bigger.hdf5\n",
            "Epoch 78/100\n",
            "154810/154810 [==============================] - 78s 502us/step - loss: 1.2748\n",
            "\n",
            "Epoch 00078: loss improved from 1.27912 to 1.27484, saving model to weights-improvement-78-1.2748-bigger.hdf5\n",
            "Epoch 79/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.2684\n",
            "\n",
            "Epoch 00079: loss improved from 1.27484 to 1.26842, saving model to weights-improvement-79-1.2684-bigger.hdf5\n",
            "Epoch 80/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.2676\n",
            "\n",
            "Epoch 00080: loss improved from 1.26842 to 1.26761, saving model to weights-improvement-80-1.2676-bigger.hdf5\n",
            "Epoch 81/100\n",
            "154810/154810 [==============================] - 78s 501us/step - loss: 1.2626\n",
            "\n",
            "Epoch 00081: loss improved from 1.26761 to 1.26259, saving model to weights-improvement-81-1.2626-bigger.hdf5\n",
            "Epoch 82/100\n",
            "154810/154810 [==============================] - 77s 500us/step - loss: 1.2598\n",
            "\n",
            "Epoch 00082: loss improved from 1.26259 to 1.25981, saving model to weights-improvement-82-1.2598-bigger.hdf5\n",
            "Epoch 83/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.2565\n",
            "\n",
            "Epoch 00083: loss improved from 1.25981 to 1.25653, saving model to weights-improvement-83-1.2565-bigger.hdf5\n",
            "Epoch 84/100\n",
            "154810/154810 [==============================] - 77s 495us/step - loss: 1.2532\n",
            "\n",
            "Epoch 00084: loss improved from 1.25653 to 1.25325, saving model to weights-improvement-84-1.2532-bigger.hdf5\n",
            "Epoch 85/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.2498\n",
            "\n",
            "Epoch 00085: loss improved from 1.25325 to 1.24981, saving model to weights-improvement-85-1.2498-bigger.hdf5\n",
            "Epoch 86/100\n",
            "154810/154810 [==============================] - 78s 505us/step - loss: 1.2455\n",
            "\n",
            "Epoch 00086: loss improved from 1.24981 to 1.24550, saving model to weights-improvement-86-1.2455-bigger.hdf5\n",
            "Epoch 87/100\n",
            "154810/154810 [==============================] - 79s 513us/step - loss: 1.2405\n",
            "\n",
            "Epoch 00087: loss improved from 1.24550 to 1.24049, saving model to weights-improvement-87-1.2405-bigger.hdf5\n",
            "Epoch 88/100\n",
            "154810/154810 [==============================] - 79s 510us/step - loss: 1.2358\n",
            "\n",
            "Epoch 00088: loss improved from 1.24049 to 1.23581, saving model to weights-improvement-88-1.2358-bigger.hdf5\n",
            "Epoch 89/100\n",
            "154810/154810 [==============================] - 78s 507us/step - loss: 1.2345\n",
            "\n",
            "Epoch 00089: loss improved from 1.23581 to 1.23454, saving model to weights-improvement-89-1.2345-bigger.hdf5\n",
            "Epoch 90/100\n",
            "154810/154810 [==============================] - 78s 505us/step - loss: 1.2322\n",
            "\n",
            "Epoch 00090: loss improved from 1.23454 to 1.23219, saving model to weights-improvement-90-1.2322-bigger.hdf5\n",
            "Epoch 91/100\n",
            "154810/154810 [==============================] - 77s 495us/step - loss: 1.2277\n",
            "\n",
            "Epoch 00091: loss improved from 1.23219 to 1.22771, saving model to weights-improvement-91-1.2277-bigger.hdf5\n",
            "Epoch 92/100\n",
            "154810/154810 [==============================] - 77s 495us/step - loss: 1.2273\n",
            "\n",
            "Epoch 00092: loss improved from 1.22771 to 1.22732, saving model to weights-improvement-92-1.2273-bigger.hdf5\n",
            "Epoch 93/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.2207\n",
            "\n",
            "Epoch 00093: loss improved from 1.22732 to 1.22072, saving model to weights-improvement-93-1.2207-bigger.hdf5\n",
            "Epoch 94/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.2223\n",
            "\n",
            "Epoch 00094: loss did not improve from 1.22072\n",
            "Epoch 95/100\n",
            "154810/154810 [==============================] - 77s 496us/step - loss: 1.2132\n",
            "\n",
            "Epoch 00095: loss improved from 1.22072 to 1.21316, saving model to weights-improvement-95-1.2132-bigger.hdf5\n",
            "Epoch 96/100\n",
            "154810/154810 [==============================] - 77s 496us/step - loss: 1.2151\n",
            "\n",
            "Epoch 00096: loss did not improve from 1.21316\n",
            "Epoch 97/100\n",
            "154810/154810 [==============================] - 77s 499us/step - loss: 1.2134\n",
            "\n",
            "Epoch 00097: loss did not improve from 1.21316\n",
            "Epoch 98/100\n",
            "154810/154810 [==============================] - 77s 498us/step - loss: 1.2097\n",
            "\n",
            "Epoch 00098: loss improved from 1.21316 to 1.20967, saving model to weights-improvement-98-1.2097-bigger.hdf5\n",
            "Epoch 99/100\n",
            "154810/154810 [==============================] - 77s 497us/step - loss: 1.2073\n",
            "\n",
            "Epoch 00099: loss improved from 1.20967 to 1.20730, saving model to weights-improvement-99-1.2073-bigger.hdf5\n",
            "Epoch 100/100\n",
            "154810/154810 [==============================] - 77s 494us/step - loss: 1.1993\n",
            "\n",
            "Epoch 00100: loss improved from 1.20730 to 1.19928, saving model to weights-improvement-100-1.1993-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3483443eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091ECGunWwJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5c68e157-1d62-46f9-bd36-60ed202b12b5"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" se went on yawning and rubbing its eyes for it was \"\n",
            " a ling of the sabbit was a little strp of her head it and alice was a large cane said the mock turtle  she cormouse went on you and fat off than said the mock turtle  the cormouse went on to be a loute there was a little strppe were the toigited his head that she was a little sime and then and then alice was a large cane said the dat and the trial and the mooy the white rabbit wery soon on said alice  why she was aeginning to say the dnok with a little shme alice was a large cane she was said a\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}